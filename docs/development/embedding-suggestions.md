# Semantic Tagging & Suggestions (pgvector)

Vekku-Bun leverages **pgvector** to provide high-performance, AI-driven tag suggestions for content.

## Core Concepts

### 1. The "Concept" vs. "Tag" Split
To optimize storage and search, we differentiate between a user's tag and the global semantic concept.

*   **Tag Concept (`tag_embeddings` table):**
    *   Represents a unique meaning (e.g., "Programming", "Java").
    *   Stores the high-dimensional vector (embedding) generated by the AI model.
    *   **ID is Deterministic:** Generated using `uuidv5(semantic)`. This acts as a global dictionary. If User A tags "Java" and User B tags "Java", they point to the same row in this table.
    *   **Uniqueness:** Stored once. We don't duplicate vectors.

*   **User Tag (`tags` table):**
    *   Represents the link between a User and a Concept.
    *   Stores the user's specific naming convention (e.g., User might type "java" or "JAVA", but it links to the concept "Java").
    *   Contains the `embeddingId` (FK to `tag_embeddings`).

### 2. Suggestion Flow

1.  **Content Creation/Update:**
    *   User posts content.
    *   `ContentService` saves the content.
    *   `ContentService` triggers `TagSuggestionService.createSuggestionsForContent`.

2.  **Vector Search (The Magic):**
    *   We generate an embedding for the *Content Body*.
    *   We perform a **User-Scoped Similarity Search** in Postgres:
        ```sql
        SELECT tags.*, embeddings.semantic
        FROM tags
        JOIN tag_embeddings ON tags.embedding_id = tag_embeddings.id
        WHERE tags.user_id = :userId
        ORDER BY tag_embeddings.embedding <=> :contentVector
        LIMIT 5;
        ```
    *   **Efficiency:** Postgres filters by the specific user's tags *first*, and then calculates distance only for that subset (or uses the HNSW index effectively). We do *not* scan the entire global vector space.

3.  **Storage (`content_tag_suggestions` table):**
    *   The top matches (below a certain distance threshold) are stored in `content_tag_suggestions`.
    *   This allows the frontend to simply `GET /api/suggestions/content/:id` without re-running the heavy AI inference every time.

## Database Schema

```typescript
// tag_embeddings (Global Dictionary)
{
  id: "uuid-v5-semantic",
  semantic: "Machine Learning",
  embedding: [0.12, -0.5, ...], // 384 dimensions
}

// tags (User Links)
{
  id: "uuid-v4",
  userId: "user-123",
  name: "ML",
  embeddingId: "uuid-v5-semantic"
}

// content_tag_suggestions (Cache)
{
  contentId: "content-abc",
  tagId: "tag-xyz", // Links to a User Tag, NOT just a generic concept
  score: "0.15" // Distance (lower is better)
}
```

## Why this architecture?

1.  **Storage Efficient:** Vectors are large. We store them once per concept, not once per user-tag.
2.  **Fast:** `pgvector` does the math in C. No moving data to the application layer.
3.  **Personalized:** We only suggest tags *the user has already defined* (or "learned"), ensuring the suggestions fit their personal organization system.
