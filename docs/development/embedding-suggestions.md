# Semantic Tagging & Suggestions (pgvector)

Vekku-Bun leverages **pgvector** to provide high-performance, AI-driven tag suggestions for content.

## Core Concepts

### 1. The "Concept" vs. "Tag" Split
To optimize storage and search, we differentiate between a user's tag and the global semantic concept.

*   **Tag Concept (`tag_embeddings` table):**
    *   Represents a unique meaning (e.g., "Programming", "Java").
    *   Stores the high-dimensional vector (embedding) generated by the AI model.
    *   **ID is Deterministic:** Generated using `uuidv5(normalized_semantic)`. This acts as a global dictionary.
    *   **Lookup Key:** The `semantic` string (normalized) acts as the bridge between user tags and embeddings.

*   **User Tag (`tags` table):**
    *   Represents the link between a User and a Concept.
    *   Stores the user's specific naming convention (e.g., User might type "java" or "JAVA").
    *   **Key Change:** Instead of a foreign key to `embeddingId`, it stores the `semantic` string directly. This decouples the tables and allows for easier data migration and independent updates.

### 2. Suggestion Flow

1.  **Content Creation/Update:**
    *   User posts content.
    *   `ContentService` saves the content.
    *   `ContentService` triggers `TagSuggestionService.createSuggestionsForContent`.

2.  **Vector Search (The Magic):**
    *   We generate an embedding for the *Content Body*.
    *   We perform a **User-Scoped Similarity Search** in Postgres:
        ```sql
        SELECT tags.*, embeddings.semantic
        FROM tags
        JOIN tag_embeddings ON tags.semantic = tag_embeddings.semantic
        WHERE tags.user_id = :userId
        ORDER BY tag_embeddings.embedding <=> :contentVector
        LIMIT 5;
        ```
    *   **Efficiency:** Postgres filters by the specific user's tags *first*, and then calculates distance only for that subset.

3.  **Storage (`content_tag_suggestions` table):**
    *   The top matches are stored in `content_tag_suggestions`.
    *   This allows the frontend to simply `GET /api/suggestions/content/:id` without re-running the heavy AI inference every time.

## Database Schema

```typescript
// tag_embeddings (Global Dictionary)
{
  id: "uuid-v5-semantic",
  semantic: "machine learning", // Normalized
  embedding: [0.12, -0.5, ...], // 384 dimensions
}

// tags (User Links)
{
  id: "uuid-v4",
  userId: "user-123",
  name: "ML",
  semantic: "machine learning" // Links to Global Dictionary via text
}

// content_tag_suggestions (Cache)
{
  contentId: "content-abc",
  tagId: "tag-xyz",
  score: "0.15" // Distance (lower is better)
}
```

## Why this architecture?

1.  **Storage Efficient:** Vectors are large. We store them once per concept, not once per user-tag.
2.  **Decoupled:** User tags are just strings. The "AI Magic" (embeddings) is an optional layer on top. If the embedding service is down, tags still work.
3.  **Fast:** `pgvector` does the math in C. No moving data to the application layer.
4.  **Personalized:** We only suggest tags *the user has already defined*, ensuring the suggestions fit their personal organization system.